# coding: utf-8 
# @æ—¶é—´   : 2021/8/3 6:01 ä¸‹åˆ
# @ä½œè€…   : æ–‡å±±
# @é‚®ç®±   : wolaizhinidexin@163.com
# @ä½œç”¨   : é€»è¾‘å›žå½’å¢žåŠ æ­£åˆ™åŒ–,é€šè¿‡åŠ å…¥æ­£åˆ™é¡¹æå‡é€»è¾‘å›žå½’ç®—æ³•ï¼Œæœ‰åŠ©äºŽé¿å…è¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚
#          è¿‡æ‹Ÿåˆå³è®­ç»ƒå‡ºæ¥çš„hð·å€¼èƒ½å¤Ÿå°†æ‰€æœ‰çš„æ ·æœ¬è¿›è¡ŒåŒºåˆ†,æ‰€å¾—åˆ°çš„æ›²çº¿æžåº¦æ‰­æ›²æ­¤æ—¶Jð·å€¼è¶‹è¿‘äºŽ0,æ— æ³•å¾ˆå¥½åœ°æ³›åŒ–åˆ°æ–°çš„æ ·æœ¬æ•°æ®.
#          å½“æˆ‘ä»¬æ ·æœ¬æ•°æ®çš„ç‰¹å¾éžå¸¸å¤š,ä½†è®­ç»ƒæ•°æ®éžå¸¸å°‘æ—¶,å°±ä¼šå‡ºçŽ°è¿‡æ‹Ÿåˆçš„æƒ…å†µ
#           1. äººå·¥é€‰æ‹©æœ€ç›¸å…³çš„å˜é‡ç‰¹å¾,ä½†æ˜¯æœ‰å¯èƒ½èˆå¼ƒä¸€äº›ç›¸å…³çš„ç‰¹å¾.
#           2. æ­£åˆ™åŒ–æ¥è¿›è¡Œå®žçŽ°. æˆ‘ä»¬ä¿ç•™æ‰€æœ‰ç‰¹å¾,ä½†å‡å°‘æ ·æœ¬çš„æ•°é‡çº§ð·jçš„å¤§å°.
# @æ•°æ®   : è®¾æƒ³ä½ æ˜¯å·¥åŽ‚çš„ç”Ÿäº§ä¸»ç®¡ï¼Œä½ æœ‰ä¸€äº›èŠ¯ç‰‡åœ¨ä¸¤æ¬¡æµ‹è¯•ä¸­çš„æµ‹è¯•ç»“æžœ
# @æ–‡ä»¶   : LogisticRegression_2.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from LogisticRegression_1 import LGR


def read_csv(isShow=True, saveName='./img/logistic_2.jpg'):
    data = pd.read_csv('./data/ex2data2.txt', names=['Test 1', 'Test 2', 'Accepted'])
    positive = data[data['Accepted'].isin([1])]
    negative = data[data['Accepted'].isin([0])]

    fig, ax = plt.subplots(figsize=(8, 5))
    ax.scatter(positive['Test 1'], positive['Test 2'], s=50, c='b', marker='o', label='Accepted')
    ax.scatter(negative['Test 1'], negative['Test 2'], s=50, c='r', marker='x', label='Rejected')
    ax.legend()
    ax.set_xlabel('Test 1 Score')
    ax.set_ylabel('Test 2 Score')
    if isShow:
        plt.show()
    else:
        plt.savefig(saveName)
    return data


def add_polynomial(x1, x2, power):
    """
    æˆ‘ä»¬çŸ¥é“z = Î¸^T * X ã€‚å‡è®¾åªæœ‰2ä¸ªå˜é‡ï¼Œåˆ™ Î¸_0 + Î¸_1 * x1 + Î¸_2 * x2
    ä»¤Î¸_0 = -3, Î¸_1 = 1,Î¸_2 = 1
    å¦‚ï¼Œåˆ™P(y=1|x:Î¸)ä¸º1
    åˆ™, -3 + x1 + x2 >0ï¼Œåˆ™x1 + x2 > 3;
    å¦‚ï¼ŒP(y=0|x:Î¸)ä¸º0ï¼Œåˆ™   x1 + x2 < 3ï¼›
    åˆ™è¿™ä¸€æ¡çº¿ï¼Œå«å†³ç­–è¾¹ç•Œçº¿ã€‚
    éœ€è¦æ³¨æ„ï¼Œå†³ç­–è¾¹ç•Œçº¿å¯èƒ½æ˜¯ä¸ªæ›²çº¿ã€‚é€šè¿‡å¤šé¡¹å¼çš„å¢žåŠ ï¼Œå¯ä»¥ä½¿å†³ç­–è¾¹ç•Œçº¿å˜æˆä¸€ä¸ªå¤šè¾¹å½¢çš„çº¿ï¼Œæ¯”å¦‚
    Î¸_0 + Î¸_1 * x1 + Î¸_2 * x2 + Î¸_3 * x1^2 + Î¸_4 * x1 * x2 + Î¸_5 * x2^2 + ...
    å¤šé¡¹å¼çš„å¢žåŠ ç»´åº¦ï¼Œå¯èƒ½ä¼šä½¿å¾—æ›²çº¿æžåº¦æ‰­æ›²ã€‚
    æ¯”å¦‚ï¼šÎ¸_0 + Î¸_1 * x1^2 + Î¸_2 * x2^2,ä»¤Î¸_0 = -1,Î¸_1=0,Î¸_2 =0,åˆ™
    -1 + x1^2 + x2^2  >0ï¼Œåˆ™y = 1
    -1 + x1^2 + x2^2  <0, åˆ™y = 0
    åˆ™x1^2 + x2^2 > 1; x1^2 + x2^2 < 1ï¼Œåˆ™æ­¤æ—¶å°±æ˜¯ä¸€ä¸ªåœ†äº†

    é«˜ç»´ç‰¹å¾å‘é‡ä¸Šè®­ç»ƒçš„logisticå›žå½’åˆ†ç±»å™¨å°†ä¼šæœ‰ä¸€ä¸ªæ›´å¤æ‚çš„å†³ç­–è¾¹ç•Œï¼Œå½“æˆ‘ä»¬åœ¨äºŒç»´å›¾ä¸­ç»˜åˆ¶æ—¶ï¼Œä¼šå‡ºçŽ°éžçº¿æ€§ã€‚
    è™½ç„¶ç‰¹å¾æ˜ å°„å…è®¸æˆ‘ä»¬æž„å»ºä¸€ä¸ªæ›´æœ‰è¡¨çŽ°åŠ›çš„åˆ†ç±»å™¨ï¼Œä½†å®ƒä¹Ÿæ›´å®¹æ˜“è¿‡æ‹Ÿåˆ.
    :return:
    """
    data = {}
    for i in np.arange(power + 1):
        for p in np.arange(i + 1):
            data["f{}{}".format(i - p, p)] = np.power(x1, i - p) * np.power(x2, p)
    return pd.DataFrame(data)


class Regularization_LRG(LGR):
    """
    ä¸ºäº†è§£å†³å‡ç»´å¸¦æ¥çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¢žåŠ æ­£åˆ™åŒ–æ¥è¿›è¡Œæ•°æ®çš„æ‹Ÿåˆ
    """

    def __init__(self, X, y):
        super().__init__(X, y)

    def hLGRReTheta(self, theta, C):
        """
        é’ˆå¯¹æŸå¤±å‡½æ•°è¿›è¡Œæ­£åˆ™åŒ–çš„æƒ©ç½š
                                 m
        Cost(hÎ¸(x),y) = -1/m * [ âˆ‘ y^i * log(hÎ¸(x^i)) + (1 - y^i) * log(1 - hÎ¸(x^i)]
                                i=1
                                    m
        JÎ¸ = Cost(hÎ¸(x),y) + l/2m * âˆ‘ Î¸_j^2
                                    j=1
        å¦‚æžœè®¾ç½®æ­£åˆ™åŒ–å‚æ•°å€¼è¿‡å¤§,ä¼šå¯¼è‡´å‡ºçŽ°ð·_1è‡³ð·_nçš„å€¼è¶‹è¿‘äºŽ0æ‰èƒ½ä½¿Jð·çš„å€¼æœ€å°åŒ–,åˆ™æ­¤æ—¶å°±å‡ºçŽ°äº†æ¬ æ‹Ÿåˆçš„æƒ…å†µ,
        å› ä¸ºæ­¤æ—¶çš„æ˜¯ð·0çš„å€¼,å³ä¸€æ¡å¹³è¡Œçº¿.
        :param theta:
        :param l:
        :return:
        """
        # ä¸å¯¹ç¬¬0ä¸ªÎ¸è¿›è¡Œæƒ©ç½š
        _theta = theta[1:]
        reg = (C / (2 * len(self.X))) * (_theta @ _theta)  # _theta@_theta == inner product
        return self.hLGRTheta(theta) + reg

    def ReGradientDescent(self, alpha, epoch, theta, C):
        """
        åŠ å…¥æ­£åˆ™åŒ–çš„æ¢¯åº¦ä¸‹é™
        :param alpha:
        :param epoch:
        :param theta:
        :return:
        """
        # reg = (C / len(X)) * theta
        # reg[0] = 0
        # theta,cost = self.gradientDescent(alpha,epoch,theta)
        # theta = theta + reg
        # return theta
        temp = np.matrix(np.zeros(theta.shape))  # åˆå§‹åŒ–ä¸€ä¸ª Î¸ ä¸´æ—¶çŸ©é˜µï¼Œç”¨æ¥å­˜å‚¨è®¡ç®—å‡ºæ¥çš„thetaå€¼
        cost = np.zeros(epoch)
        m = self.X.shape[0]
        for i in range(epoch):
            temp = theta * (1 - (alpha * (C / m))) - (alpha * (1 / m)) * ((self.sigmoid(self.X @ theta) - self.y) @ self.X)
            theta = temp
            cost[i] = self.hLGRReTheta(theta,C)
        return theta, cost


def verify_reg(final_theta):
    """
    å†³ç­–è¾¹ç•Œä¸ºä¸€ä¸ªæ¤­åœ†
    :param final_theta:
    :return:
    """
    x = np.linspace(-1, 1.5, 250)
    xx, yy = np.meshgrid(x, x)

    z = add_polynomial(xx.ravel(), yy.ravel(), 6).values
    z = z @ final_theta
    z = z.reshape(xx.shape)

    data = pd.read_csv('./data/ex2data2.txt', names=['Test 1', 'Test 2', 'Accepted'])
    positive = data[data['Accepted'].isin([1])]
    negative = data[data['Accepted'].isin([0])]

    fig, ax = plt.subplots(figsize=(8, 5))
    ax.scatter(positive['Test 1'], positive['Test 2'], s=50, c='b', marker='o', label='Accepted')
    ax.scatter(negative['Test 1'], negative['Test 2'], s=50, c='r', marker='x', label='Rejected')
    ax.legend()
    ax.set_xlabel('Test 1 Score')
    ax.set_ylabel('Test 2 Score')

    plt.contour(xx, yy, z, 0)
    plt.ylim(-.8, 1.2)
    plt.show()


if __name__ == "__main__":
    data = read_csv(isShow=False)
    x1 = data['Test 1'].values
    x2 = data['Test 2'].values
    y = data['Accepted'].values
    # å¯¹æºæ•°æ®è¿›è¡Œå‡ç»´
    data = add_polynomial(x1, x2,6)
    if 'Ones' not in data.columns:
        data.insert(0, 'Ones', 1)
    X = data.iloc[:, :-1].values

    # theta = np.zeros(X.shape[1])
    # l = LGR(X, y)
    # final_theta, cost = l.gradientDescent(0.0000001, 200000, theta)
    # print(l.hLGRTheta(final_theta), final_theta)

    theta = np.zeros(X.shape[1])
    l = Regularization_LRG(X,y)
    a = 0.01
    final_theta, cost = l.ReGradientDescent(0.003,20000,theta,a)

    # final_theta = np.array([0.57761135, 0.47056293, 1.09213933, -0.93555548, -0.15107417,
    #                         -0.96567576, -0.49622178, -0.87226365, 0.5986215, -0.47857791,
    #                         -0.19652206, -0.10212812, -0.1513566, -0.03407832, -1.868297,
    #                         -0.25062387, -0.49045048, -0.20293012, -0.26033467, 0.02385201,
    #                         -0.0290203, -0.0543879, 0.01131411, -1.39767636, -0.16559351,
    #                         -0.24745221, -0.29518657, 0.00854288])
    print(min(cost))
    print(l.hLGRReTheta(final_theta,a), final_theta)
    # verify_reg(final_theta)
