# coding: utf-8 
# @时间   : 2021/8/14 4:35 下午
# @作者   : 文山
# @邮箱   : wolaizhinidexin@163.com
# @作用   : 神经网络
# @文件   : NeuralNetworks_1.py

"""
神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。
这些神经元 (也叫激活单元，activation unit)采纳一些特征作为输出，并且根据本身的模型提供一个输出。

〔x_1〕     ○〔a_1^2〕
      ↗︎                 ↘︎
〔x_2〕->   ○〔a_2^2〕 ⟶ ⟶ ○ ⟶ ⟶ h𝝷(X)
      ↘︎                 ↗
〔x_3〕     ○〔a_3^2〕
 输入层     隐藏层        输出层
 ------------------------------------------------------------------
 X输入单元, a_1称之为隐藏单元，最后是输出单元h𝝷(X);上图，每个X均会有3个传播方向。
 只要在输出层也输出层中的都称之为隐藏层。每一层的输出变量都是下一层的输入变量。
 ------------------------------------------------------------------
 因a_2层，存在n3 * x3+1(偏置项0)个。 a_3则有1 * n3+1 ，则我们获取的传播路径有 n * a + 1
 ------------------------------------------------------------------
 a_i^j代表第j层第i个激活单元；𝝷^j代表从第j层映射到j+1层时的权重的矩阵。
 则,
 a_1^2 = g(𝝷_1_0^1 * x_0 + 𝝷_1_1^1 * x_1 + 𝝷_1_2^1 * x_2 + 𝝷_1_3^1 * x_3 )
 a_2^2 = g(𝝷_2_0^1 * x_0 + 𝝷_2_1^1 * x_1 + 𝝷_2_2^1 * x_2 + 𝝷_2_3^1 * x_3 )
 a_3^2 = g(𝝷_3_0^1 * x_0 + 𝝷_3_1^1 * x_1 + 𝝷_3_2^1 * x_2 + 𝝷_3_3^1 * x_3 )

 输出层，
 h𝝷(𝑥) = g(𝝷_1_0^2 * a_0^2 + 𝝷_1_1^2 * a_1^2 + 𝝷_1_2^2 * a_2^2 + 𝝷_1_3^2 * a_3^2)
 每一个𝑎都是由上一层所有的𝑥和每一个𝑥所对应𝝷决定的,我们将这个算法称之为前向传播算法FORWARD PROPAGATION。
 ------------------------------------------------------------------
 向量化的表示方法，则令。

     [ x_0           [  z_0^2
       x_1              z_1^2
 X =   x_2      Z^2 =   z_2^2
       x_3              z_3^2
     ]               ]


     [                                        [
       𝝷_1_0^1  𝝷_1_1^1  𝝷_1_2^1    𝝷_1_3^      x_0
 g *   𝝷_2_0^1  𝝷_2_1^1  𝝷_2_2^1    𝝷_2_3^1  *  x_1
       𝝷_3_0^1  𝝷_3_1^1  𝝷_3_2^1    𝝷_3_3^1     x_2
                                               x_3
    ]                                         ]
 则上面的计算过程实质为：
        [                                                                   [
            𝝷_1_0^1 * x_0 + 𝝷_1_1^1 * x_1 + 𝝷_1_2^1 * x_2 + 𝝷_1_3^1 * x_3       a_1^2
 g *        𝝷_2_0^1 * x_0 + 𝝷_2_1^1 * x_1 + 𝝷_2_2^1 * x_2 + 𝝷_2_3^1 * x_3  =    a_2^2
            𝝷_3_0^1 * x_0 + 𝝷_3_1^1 * x_1 + 𝝷_3_2^1 * x_2 + 𝝷_3_3^1 * x_3       a_3^2
        ]                                                                   ]
 令, Z^2 = 𝝷^1 * X, 则a^2 = g(Z^2)，计算后添加a_0^2 = 1，则计算h𝝷(𝑥) 的输出为：
                                                [
                                                  a_0^2
                                                  a_1^2
 g * [𝝷_1_0^2   𝝷_1_1^2  𝝷_1_2^2    𝝷_1_3^2] *     a_2^2
                                                  a_3^2
                                                ]
 则上面的计算h𝝷(𝑥) 过程实质为：
 g * (𝝷_1_0^2 * a_0^2 + 𝝷_1_1^2 * a_1^2 + 𝝷_1_2^2 * a_2^2 + 𝝷_1_3^2 * a_3^2) = h𝝷(𝑥)

 令Z^3 = 𝝷^2 * a^2，则h𝝷(𝑥) = a^3 = g(Z^3 )
 ------------------------------------------------------------------
 综上，其实神经网络就像是 logistic regression，只不过我们把 logistic regression 中的输入向量 [𝑥_1 ∼ 𝑥_3 ]
 变成了中间层的[𝑎_1^2 ∼ 𝑎_3^2], 即:
 h𝝷(𝑥) = g * (𝝷_1_0^2 * a_0^2 + 𝝷_1_1^2 * a_1^2 + 𝝷_1_2^2 * a_2^2 + 𝝷_1_3^2 * a_3^2)

 我们可以把𝑎0, 𝑎1, 𝑎2, 𝑎3看成更为高级的特征值，也就是𝑥0, 𝑥1, 𝑥2, 𝑥3的进化体，并且它们是由𝑥与决定的，
 因为是梯度下降的(每一层的𝝷值是由梯度下降获取的），所以𝑎是变化的，并且变得越来越厉害，
 所以这些更高级的特征值远比仅仅将𝑥次方厉害，也能更好的预测新数据。

 ------------------------------------------------------------------
 逻辑与(AND), x1,x2 只能在(0,1)中选择，y = x1 and x2

〔1〕
         ↘︎
〔x_2〕 ⟶ ⟶ ○ ⟶ ⟶ h𝝷(X)
         ↗
〔x_3〕

 假设𝝷_1_0^1 = -30, 𝝷_1_2^1 = 20, 𝝷_1_3^1 = 20,则h𝝷(X) = g ( -30 + 20 * x1 + 20 * x2)，因于是一个二分类问题，所以
 x1     x2    |  h𝝷(X)
 0      0     |  g(-30) ≈ 0
 0      1     |  g(-10) ≈ 0
 1      0     |  g(-10) ≈ 0
 1      1     |  g(10)  ≈ 1

 如果是多分类问题，比如4个分类，则输出的结果可能是以下。
 如果 我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有4个值

        ○   ○
  ○     ○   ○   ○
  ○     ○   ○   ○
  ○     ○   ○   ○
                ○
        ○   ○

 [
   1 0 0 0
   0 1 0 0
   0 0 1 0
   0 0 0 1
 ]
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.io import loadmat


def sigmoid(z):
    return 1 / (1 + np.exp(-z))


def load_weight():
    """获得theta值"""
    data = loadmat('./data/ex3weights.mat')
    return data['Theta1'], data['Theta2']


def load_data():
    """原始数据"""
    data = loadmat('./data/ex3data1.mat')
    X = data['X']
    y = data['y']
    y = y.flatten()
    # 增加偏置项
    X = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1)  # intercept
    return X, y


def z2(X, theta1):
    """ Z^2 = 𝝷^1 * X """
    a1 = X
    z2 = a1 @ theta1.T
    return z2


def z3(z2, theta2):
    z2 = np.insert(z2, 0, 1, axis=1)
    # a^2 = g(Z^2)
    a2 = sigmoid(z2)
    # Z^3 = 𝝷^2 * a^2
    z3 = a2 @ theta2.T
    return z3


def a4(z3, y):
    """输出层，得到最后的预测值"""
    # a^3 = g(Z^3)
    a3 = sigmoid(z3)
    y_pred = np.argmax(a3, axis=1) + 1
    accuracy = np.mean(y_pred == y)
    print('accuracy = {0}%'.format(accuracy * 100))  # accuracy = 97.52%


if __name__ == "__main__":
    """前向传播的演示。theta1,theta2的值已获取到ex3weights.mat文件中"""
    theata = load_weight()
    # X值，第1层的值
    X, y = load_data()
    # 隐藏层中第2层的计算
    z2 = z2(X, theata[0])
    # 隐藏层中第3层的计算
    z3 = z3(z2, theata[1])
    # 输出层的结果
    a4(z3, y)
