# coding: utf-8 
# @æ—¶é—´   : 2021/8/2 8:32 ä¸‹åˆ
# @ä½œè€…   : æ–‡å±±
# @é‚®ç®±   : wolaizhinidexin@163.com
# @ä½œç”¨   : é€»è¾‘å›žå½’
# @æ•°æ®   : å‡è®¾ä½ æ˜¯ä¸€æ‰€å¤§å­¦çš„è¡Œæ”¿ç®¡ç†äººå‘˜ï¼Œä½ æƒ³æ ¹æ®ä¸¤é—¨è€ƒè¯•çš„ç»“æžœï¼Œæ¥å†³å®šæ¯ä¸ªç”³è¯·äººæ˜¯å¦è¢«å½•å–
# @æ–‡ä»¶   : LogisticRegression_1.py

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


def read_csv(isShow=True, saveName='./img/logistic_1.jpg'):
    """
    è¯»å–åŽŸæ•°æ®
    :return:
    """
    data = pd.read_csv('./data/ex2data1.txt', names=['exam1', 'exam2', 'admitted'])
    # å›¾åž‹åŒ–å±•ç¤ºå…¶æ•°æ®
    positive = data[data.admitted == 1]  # 1
    negetive = data[data.admitted == 0]  # 0
    fig, ax = plt.subplots(figsize=(6, 5))
    # æ•£åˆ—å›¾
    ax.scatter(positive['exam1'], positive['exam2'], c='b', label='Admitted')
    ax.scatter(negetive['exam1'], negetive['exam2'], s=50, c='r', marker='x', label='Not Admitted')
    # è®¾ç½®å›¾ä¾‹æ˜¾ç¤ºåœ¨å›¾çš„ä¸Šæ–¹
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width, box.height * 0.8])
    ax.legend(loc='center left', bbox_to_anchor=(0.2, 1.12), ncol=3)
    # è®¾ç½®æ¨ªçºµåæ ‡å
    ax.set_xlabel('Exam 1 Score')
    ax.set_ylabel('Exam 2 Score')
    if isShow:
        plt.show()
    else:
        plt.savefig(saveName)
    return data


class LGR():
    """
    1, ä¸ºä»€ä¹ˆè¦ç”¨é€»è¾‘å›žå½’ï¼Ÿ
       ä»Žå›¾ä¾‹ä¸­å¯ä»¥çœ‹å‡ºæ¥ä¼¼ä¹Žç”¨çº¿åž‹å›žå½’ä¹Ÿå¯ä»¥ç”¨ä¸€æ¡çº¿è¿›è¡ŒåŒºåˆ†ï¼Œä½†æ˜¯ç”±äºŽhð·(Ï‡i) = Î¸^T * X = Î¸_0 * x_0 + Î¸_1 * x_1+Î¸_2 * x_2 + ..Î¸_n * x_n
    çš„å€¼ï¼Œå¯èƒ½æ˜¯å¤§äºŽ1æˆ–è€…å°äºŽ0çš„ï¼Œè€Œæˆ‘ä»¬æºæ•°æ®çš„å€¼åªèƒ½æ˜¯åœ¨[0,1]ä¹‹é—´çš„èŒƒå›´ï¼Œä¸”>0.5ä¸º1ï¼Œ<0.5ä¸º0ï¼Œæ‰€ä»¥æ˜¯ä¸é€‚ç”¨çš„ã€‚
    2, ä¸ºä»€ä¹ˆsigmoidå‡½æ•°å¯ä»¥ç”¨æ¥è¿›è¡Œé€»è¾‘å›žå½’ï¼Ÿ
    """

    def __init__(self, X, y):
        self.X = X
        self.y = y

    def sigmoid(self, z):
        """
        é€»è¾‘å›žå½’çš„æ¿€æ´»å‡½æ•°ï¼Œå…¶ä¸­z = Î¸^T * X = Î¸_0 * x_0 + Î¸_1 * x_1+Î¸_2 * x_2 + ..Î¸_n * x_n
        :param z:
        :return:
        """
        return 1 / (1 + np.exp(- z))

    def hLGRTheta(self, theta):
        """
        é€»è¾‘å›žå½’çš„æŸå¤±å‡½æ•°ä¸º
                        {
                            -log(hÎ¸(x)), if y == 1
        Cost(hÎ¸(x),y) =     -log(1 - hÎ¸(x)), if y == 0
                        }
        verfiy_log()å‡½æ•°è¡¨æ˜Žï¼š
        å¦‚æžœæˆ‘ä»¬é¢„æµ‹çš„hð·(x) = 0 ,ä½†æ˜¯å®žé™…å€¼y = 1,é‚£ä¹ˆcostçš„æŸå¤±ä¼šè¶‹äºŽæ— ç©·. å¦‚æžœhð·(x)=1,å¹¶ä¸”y=1,é‚£ä¹ˆå…¶costä¼šè¶‹äºŽ0.
        æ‰€ä»¥è¿™ä¸ªå‡½æ•°éžå¸¸é€‚åˆç”¨æ¥åšé€»è¾‘å›žå½’çš„æŸå¤±å‡½æ•°

        åˆ©ç”¨æžå¤§ä¼¼ç„¶æ³•çš„ç®—æ³•ï¼Œåˆ™è®¡ç®—çš„å…¬å¼åˆäºŒä¸ºä¸€è¡¨ç¤ºä¸º
                                 m
        Cost(hÎ¸(x),y) = 1/m * [ âˆ‘ -y^i * log(hÎ¸(x^i)) - (1 - y^i) * log(1 - hÎ¸(x^i)]
                                i=1

        åˆç­‰äºŽ                    m
        Cost(hÎ¸(x),y) = -1/m * [ âˆ‘ y^i * log(hÎ¸(x^i)) + (1 - y^i) * log(1 - hÎ¸(x^i)]
                                i=1

        æ‰€ä»¥JÎ¸ = Cost(hÎ¸(x),y)
        :param theta:
        :return:
        """
        first = (-self.y) * np.log(self.sigmoid(self.X @ theta))
        second = (1 - self.y) * np.log(1 - self.sigmoid(self.X @ theta))
        # 1/m * (-A - B) = -1/m (A + B)
        result = np.mean(first - second)
        return result

    def gradientDescent(self, alpha, epoch, theta):
        """
        æ¢¯åº¦ä¸‹é™
        :return:
        """
        temp = np.matrix(np.zeros(theta.shape))  # åˆå§‹åŒ–ä¸€ä¸ª Î¸ ä¸´æ—¶çŸ©é˜µï¼Œç”¨æ¥å­˜å‚¨è®¡ç®—å‡ºæ¥çš„thetaå€¼
        cost = np.zeros(epoch)
        m = self.X.shape[0]
        for i in range(epoch):
            temp = theta - alpha  * ((self.sigmoid(self.X @ theta) - self.y) @ self.X)
            theta = temp
            cost[i] = self.hLGRTheta(theta)
        return theta, cost

    def predict(self, theta, X):
        """
        å½“hÎ¸(x)>0.5æ—¶ï¼Œåˆ™é¢„æµ‹ä¸º1;å¦åˆ™ä¸º0;
        :param theta:
        :return:
        """
        probability = self.sigmoid(X @ theta)
        return [1 if x >= 0.5 else 0 for x in probability]

    def veriy_LRG_correct(self, theta, X, y):
        """
        åœ¨éªŒè¯é›†ä¸­éªŒè¯å…¶å‡†ç¡®çŽ‡æ˜¯å¤šå°‘
        :param theta:
        :param X:
        :return:
        """
        predictions = self.predict(theta, X)
        correct = [1 if a == b else 0 for (a, b) in zip(predictions, y)]
        accuracy = sum(correct) / len(X)
        return accuracy


def verfiy_sigmoid(isShow=True, saveName='./img/logistic_verfiy_sigmoid.jpg'):
    """
    éªŒè¯ä¸ºä»€ä¹ˆç”¨sigmoidå‡½æ•°ï¼Œä»Žå›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºè¿™ä¸ªå‡½æ•°çš„å›¾åƒå˜ä¸º,å½“z>0æ—¶,yä¸º[0.5,1]
    å½“z<0æ—¶,yä¸º[0,0.5],æ‰€ä»¥0.5ä¸ºåˆ†å‰²åŽä¸€æ¡æ›²çº¿,zæ— ç©·å¤§æ—¶,è¶‹è¿‘äºŽ1,åä¹‹åˆ™è¶‹è¿‘äºŽ0.
    å¾ˆé€‚åˆç”¨æ¥åšé€»è¾‘å›žå½’çš„æ¿€æ´»å‡½æ•°
    :return:
    """
    t = LGR(0, 0)
    x1 = np.arange(-10, 10, 0.1)
    plt.plot(x1, t.sigmoid(x1), c='r')
    if isShow:
        plt.show()
    else:
        plt.savefig(saveName)


def verfiy_log(isShow=True, saveName='./img/logistic_verfiy_z.jpg'):
    x1 = np.arange(0.01, 1, 0.01)
    plt.plot(x1, -np.log(x1), c='r', )
    plt.plot(x1, np.log(1 - x1), c='b')
    # ç»˜åˆ¶ç½‘æ ¼
    plt.grid(alpha=0.4, linestyle=':')
    if isShow:
        plt.show()
    else:
        plt.savefig(saveName)

def verfiy_result(final_theta):
    """
    éªŒè¯å¾—åˆ°çš„final_thetaï¼Œåœ¨å›¾å½¢ä¸­çš„åŒºåˆ†æ˜¯æ€Žæ ·çš„
    :param final_theta:
    :return:
    """
    x1 = np.arange(130, step=0.1)
    x2 = -(final_theta[0] + x1 * final_theta[1]) / final_theta[2]
    data = pd.read_csv('./data/ex2data1.txt', names=['exam1', 'exam2', 'admitted'])
    # å›¾åž‹åŒ–å±•ç¤ºå…¶æ•°æ®
    positive = data[data.admitted == 1]  # 1
    negetive = data[data.admitted == 0]  # 0

    fig, ax = plt.subplots(figsize=(8, 5))
    ax.scatter(positive['exam1'], positive['exam2'], c='b', label='Admitted')
    ax.scatter(negetive['exam1'], negetive['exam2'], s=50, c='r', marker='x', label='Not Admitted')
    ax.plot(x1, x2)
    ax.set_xlim(0, 130)
    ax.set_ylim(0, 130)
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_title('Decision Boundary')
    plt.show()


if __name__ == "__main__":
    # data = read_csv(isShow=False)
    #################################################################################
    # éªŒè¯ä¸ºä»€ä¹ˆæ˜¯sigmoidè¿™ä¸ªå‡½æ•°
    verfiy_sigmoid()

    #################################################################################
    # éªŒè¯ä¸ºä»€ä¹ˆæŸå¤±å‡½æ•°è¦ç”¨log()
    # verfiy_log()
    #################################################################################
    # add a ones column - this makes the matrix multiplication work out easier
    # if 'Ones' not in data.columns:
    #     data.insert(0, 'Ones', 1)
    # # set X (training data) and y (target variable)
    # data = data.sample(frac=0.6)
    # X = data.iloc[:, :-1].values  # Convert the frame to its Numpy-array representation.
    # y = data.iloc[:, -1].values  # Return is NOT a Numpy-matrix, rather, a Numpy-array.
    # #################################################################################
    # theta = np.zeros(X.shape[1])
    # l = LGR(X, y)
    # final_theta, cost = l.gradientDescent(0.00001, 200000, theta)
    # print(l.hLGRTheta(final_theta),final_theta)
    # # #################################################################################
    # # # å°†æ•°æ®ç”¨æ¥åšéªŒè¯
    # data = data.sample(frac=0.2)
    # X = data.iloc[:, :-1].values
    # y = data.iloc[:, -1].values
    # print(l.veriy_LRG_correct(final_theta, X, y))
    # #################################################################################
    # å›¾å½¢åŒ–éªŒè¯
    # verfiy_result(np.array([-7.45017822 , 0.06550395 , 0.05898701]))


